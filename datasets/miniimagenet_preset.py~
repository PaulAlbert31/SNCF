import torchvision as tv
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset
import os
import csv
from tqdm import tqdm
from mypath import Path
import json
import time

class MiniImagenet84(Dataset):
    # including hard labels & soft labels
    def __init__(self, data, labels, transform=None, target_transform=None, unsup=False, transform_unsup=None, contrastive=False):
        self.data, self.targets =  data, labels
        self.transform = transform
        self.target_transform = target_transform
        self.transform_unsup = transform_unsup
        self.clean_noisy = None
        self.num_classes = 100
        self.unsup = unsup
        self.contrastive = contrastive
        self.pairs = None
        
    def __getitem__(self, index):
        img, target = self.data[index], self.targets[index]
        img = Image.open(img)#.convert('RGB')
        
        if self.transform is not None:
            img_t = self.transform(img)
            if self.contrastive:
                img_ = self.transform(img)
            else:
                img_ = img_t
                
        if self.target_transform is not None:
            target = self.target_transform(target)
            
        sample = {'image':img_t, 'image_':img_, 'target':target, 'index':index}
        if self.unsup:
            sample['image1'] = self.transform_unsup(img)
            sample['image2'] = self.transform_unsup(img)
        if self.clean_noisy is not None:
            sample['clean_noisy'] = self.clean_noisy[index]
        return sample

    def __len__(self):
        return len(self.data)


def make_dataset(root=Path.db_root_dir('miniimagenet_preset'), noise_ratio="0.3", noise_type="red"):
    np.random.seed(42)
    nclass = 100
    img_paths = []
    labels = []
    clean_noisy = []
    clean_anno = json.load(open(os.path.join(root, "mini-imagenet-annotations.json")))["data"]
    anno_dict = {}
    for anno in clean_anno:
        anno_dict[anno[0]['image/id']] = int(anno[0]['image/class/label/is_clean'])
    
    for split in ["training", "validation"]:
        if split == "training":
            class_split_path = os.path.join(root, split, '{}_noise_nl_{}'.format(noise_type, noise_ratio))
        else:
            train_num = len(img_paths)
            class_split_path = os.path.join(root, split)
        for c in range(nclass):
            class_img_paths = os.listdir(os.path.join(class_split_path, str(c)))
            class_img_paths.sort()
            for paths in class_img_paths:
                if paths[0] != "n" and split == "training":
                    clean_noisy.append(anno_dict[paths.replace(".jpg","")])
                elif split == "training":
                    clean_noisy.append(1)
                img_paths.append(os.path.join(class_split_path, str(c), paths))
                labels.append(c)

    labels = np.array(labels)
    train_paths = img_paths[:train_num]
    train_labels = labels[:train_num]
    val_paths = img_paths[train_num:]
    val_labels = labels[train_num:]
    clean_noisy = torch.tensor(clean_noisy, dtype=torch.bool)
    #clean_noisy = torch.load("clean_noisy.pth.tar")
    print(clean_noisy.sum() / len(clean_noisy))
    return train_paths, train_labels, val_paths, val_labels, None, None, clean_noisy
